We developed a robotic data acquisition system to generate ground truth data for the EEG cap electrode's position using the universal robotics UR-3 model, Microsoft azure Kinect, atracsys fusionTrac 500 camera. A robust software, integrating all the hardware has been implemented using open source software/libraries like ROS, MoveIt, and OpenCV, etc. We perform camera calibration, hand-eye calibration using a chessboard and a reflective marker before localizing the electrodes. A phantom head wearing the EEG cap is mounted on to a robot and a head coordinate system using anatomical features of the phantom face is created. While mapping the electrodes, the introduction of a common reference frame enabled us to map all the electrodes especially those at the back of the phantom head. We were able to cover all the electrodes at the cost of a measurement error of $\pm$5mm. This measurement error can be avoided by redesigning the reflective marker that can reach all the electrodes without having to move the phantom head. To evaluate the extent to which electrodes position varies at different wearing instances of the EEG cap, we generated data for 5 such instances. Finally, the phantom head is moved along specified trajectories using a robot while we record a video (30-45 seconds, 30fps) of the entire movement along with other important information. A Large amount of RGB images from the video can be used to train neural networks to detect electrodes by labeling all the electrodes in the images. Not all the electrodes can be seen in a single RGB image, on the other hand, the position of all the electrode is recorded in the Kinect frame even the ones that are not visible. This could pose a problem while creating a label to train the neural networks, therefore there is a scope for developing a filtering system to filter all the invisible electrodes positional information. These generated ground truth data can be used to evaluate any camera-based electrode detection and position estimation algorithms.